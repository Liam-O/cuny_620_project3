{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Classification of Gender based on Names\n",
    "\n",
    "This is a collaborative project conducted by the Fall 2017 students of DATA 620 at The City University of New York, in partial fulfillment of the requirements for the MS in Data Science degree.\n",
    "\n",
    "### Problem Description\n",
    "\n",
    "Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can. Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6900 words for the training set. Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set. How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?\n",
    "\n",
    "Source: Natural Language Processing with Python, exercise 6.10.2.\n",
    "\n",
    "### Contributors Include\n",
    "\n",
    "* K. Joy Payton\n",
    "* Keith Folsom\n",
    "* Sonya Hong\n",
    "* Shyam Balagurumurthy Viswanathan\n",
    "* Derek Nokes\n",
    "* Liam Byrne\n",
    "* Latif Masud\n",
    "* Valerie Briot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Corpus\n",
    "\n",
    "Note: If not already executed, nltk.download() will allow you access to the names corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages\n",
    "\n",
    "Since we all used Anaconda's Python version which comes pre-installed with most of the packages we need, we can simply import them into our notebook. `textstat` doesn't come with the base insallation, so we have to download it before importing it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textstat in c:\\users\\latif\\anaconda3\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "! pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing required libraries/packages\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.metrics import *\n",
    "import re\n",
    "    \n",
    "import operator\n",
    "import string\n",
    "from textstat.textstat import textstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we will use NLTK's provided library of male and female names, and shuffle our dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 7944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Caro', 'female'),\n",
       " ('Collie', 'female'),\n",
       " ('Geoffrey', 'male'),\n",
       " ('Kizzee', 'female'),\n",
       " ('Issy', 'female'),\n",
       " ('Dody', 'female'),\n",
       " ('Cari', 'female'),\n",
       " ('Abigale', 'female'),\n",
       " ('Rice', 'male')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ([(name, 'male') for name in names.words('male.txt')] + \\\n",
    "         [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "random.shuffle(names)\n",
    "\n",
    "print (\"Size of dataset:\", len(names))\n",
    "# let's see what the randomly shuffles names look like\n",
    "names[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Subsets\n",
    "Now that we have our overall dataset, we will split out our data into three different subsets to be used for different purposes. There are 7944 names in the dataset, and we will first split the \"names\" data set into a Development set of 7,444 entries and a Test set of 500 entries.\n",
    "\n",
    "The Development set will be used to test each features as we build the module. This test will be split between a training set and a dev-test set.  \n",
    "\n",
    "##### Development set:\n",
    "* 6944 names for the training set\n",
    "* 500 names for the dev-test set  \n",
    "\n",
    "##### Test set:\n",
    "* 500 names for the testing set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the names set into a Development set and a Test set;\n",
    "# Developmeent set will be used for training and testing each features as we build the model\n",
    "\n",
    "test_names, development_set_names = names[0:500], names[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development Set = 7444\n",
      "Test Set = 500\n"
     ]
    }
   ],
   "source": [
    "# Confirm the size of the three subsets\n",
    "print(\"Development Set = {}\".format(len(development_set_names)))\n",
    "print(\"Test Set = {}\".format(len(test_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor Class\n",
    "\n",
    "This section below is to incrementallly improve the feature extraction functions which are subsequently applied to the development and test datasets.\n",
    "\n",
    "To facilitate the analysis, a class for the classifier was constructed. This class will have all the features and various methods, show error, display confusion matrix, evaluate fit of the model. The features that the class can evaluate are: \n",
    "\n",
    "1. Last Letter\n",
    "2. First Letter, most names begining with a vowel are associated with females  \n",
    "3. Vowels count\n",
    "4. Hard consonants using general rules of c and g\n",
    "5. Soft consonants using general rules of c and g\n",
    "6. Syllable Count of names via textstat \n",
    "7. Name length\n",
    "8. Last two letters\n",
    "9. Last 3 letters\n",
    "10. Character count\n",
    "11. Character present\n",
    "12. Count of each letter\n",
    "13. Count of pair of letters in the alphabet\n",
    "14. First 2 letters\n",
    "15. First letter, Last Letter, Last 2 Letters, last 3 Letters, 2-grams\n",
    "16. Takes into account all of the top indicators for classification. \n",
    "\n",
    "There are three additional functions provided in the class, which do the following:\n",
    "* `show_errors`: prints out the number of correct idenfifications, and the number of guesses for each name passed to the function. \n",
    "* `classifier_report`: Provides a Confusion Matrix of the prediction, which helps describe the performance of our classification model.\n",
    "* `fit_model`\" Takes in a model, and a range of feature numbers, and runs the dataset through those features and reports the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP_Classifier():    \n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def get_features(self,name,feat_num):\n",
    "        '''\n",
    "        Parameters:\n",
    "            name - string of name to extract feature\n",
    "            feat_num - itterable collection of integers specifying features. *Defaults to 1:9 inclusive\n",
    "                1: last letter\n",
    "                2: first letter\n",
    "                3: Vowel counts\n",
    "                4: Hard consonant count\n",
    "                5: Soft consonant count\n",
    "                6: Syllable Count\n",
    "                7: Name length\n",
    "                8: Last two chars\n",
    "                9: Last three chars\n",
    "                10: char count --> feature for all alpha chars\n",
    "                11: char present --> feature for all alpha chars (boolean)\n",
    "        Returns:\n",
    "            features: a dictionary of extracted features\n",
    "        '''\n",
    "        features = {}    \n",
    "        \n",
    "        # Converts feat_num to itterable if type is int\n",
    "        if type(feat_num) is int:\n",
    "            feat_num = (0, feat_num)        \n",
    "       \n",
    "        # Gender Feature 1: Last letter - book example\n",
    "        if 1 in feat_num:\n",
    "            features['last_letter'] = name[-1].lower()\n",
    "            \n",
    "        # Gender Feature 2: First letter - most names beginning with a vowel --> females\n",
    "        if 2 in feat_num:\n",
    "            features['first_letter'] = name[0].lower()\n",
    "            \n",
    "        # Gender Feature 3: Vowel Counts\n",
    "        if 3 in feat_num:\n",
    "            features['vowel_count'] = len(re.sub(r'[^aeiou]', '', name.lower()))\n",
    "            \n",
    "        # Gender Feature 4: Hard consonants using general rules of c and g\n",
    "        if 4 in feat_num:\n",
    "            features['hard_consts'] = len(re.findall(r'[cg][^eiy]', name.lower()))/2\n",
    "            \n",
    "        # Gender Feature 5: Soft consonants using general rules of c and g\n",
    "        if 5 in feat_num:\n",
    "            features['soft_consts'] = len(re.findall(r'[cg][eiy]', name.lower()))/2\n",
    "            \n",
    "        # Gender Feature 6: Syllable Count of names via textstat\n",
    "        if 6 in feat_num:\n",
    "            features['syllable_count'] = textstat.syllable_count(name.lower())\n",
    "    \n",
    "        # Gender Feature 7: Name length\n",
    "        if 7 in feat_num:\n",
    "            features[\"length\"] = len(name)\n",
    "        \n",
    "        # Gender Feature 8: Last two chars\n",
    "        if 8 in feat_num:\n",
    "            features[\"last2letters\"] = name[-2:].lower()\n",
    "            \n",
    "        # Gender Feature 9: Last three chars\n",
    "        if 9 in feat_num:\n",
    "            features[\"last3letters\"] = name[-3:].lower()\n",
    "    \n",
    "        # Gender Feature 10: Char Counts (overfitts)\n",
    "        if 10 in feat_num:\n",
    "            for letter in string.ascii_lowercase:\n",
    "                features[\"count_{0}\".format(letter)] = name.lower().count(letter)\n",
    "                \n",
    "        # Gender Feature 11: Char Booleans (overfitts)\n",
    "        if 11 in feat_num:\n",
    "            for letter in string.ascii_lowercase:\n",
    "                features[\"has_{0}\".format(letter)] = letter in name.lower()\n",
    "        \n",
    "        \n",
    "        if 12 in feat_num:\n",
    "            features = {}\n",
    "            letters=list(map(chr, range(ord('a'), ord('z') + 1)))\n",
    "            for letter in letters:\n",
    "                features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "\n",
    "\n",
    "        if 13 in feat_num:\n",
    "            features = {}\n",
    "            letters=list(map(chr, range(ord('a'), ord('z') + 1)))\n",
    "            for letter1 in letters:\n",
    "                for letter2 in letters:\n",
    "                    features[\"has(\"+letter1+letter2+\")\"] = (letter1+letter2 in name.lower())\n",
    "\n",
    "        if 14 in feat_num:\n",
    "            features[\"first2Letters\"]=name[0:2].lower()\n",
    "\n",
    "        if 15 in feat_num:\n",
    "            features = {}\n",
    "            features[\"firstletter\"] = name[0].lower()\n",
    "            features[\"lastletter\"] = name[-1].lower()\n",
    "            features[\"last2letter\"] = name[-2:].lower()\n",
    "            features[\"last3letter\"] = name[-3:].lower()\n",
    "\n",
    "            letters=list(map(chr, range(ord('a'), ord('z') + 1)))\n",
    "            for letter1 in letters:\n",
    "                features[\"count(\"+letter1+\")\"] = name.lower().count(letter1)\n",
    "                features[\"has(\"+letter1+\")\"] = (letter1 in name.lower())\n",
    "                # iterate over 2-grams\n",
    "                for letter2 in letters:\n",
    "\n",
    "                    features[\"has(\"+letter1+letter2+\")\"] = (letter1+letter2 in name.lower())\n",
    "\n",
    "\n",
    "        if 16 in feat_num:\n",
    "            # define features\n",
    "            features = {}\n",
    "            # has(fo) = True\n",
    "            features[\"has(fo)\"] = ('fo' in name.lower())\n",
    "            # has(hu) = True\n",
    "            features[\"has(hu)\"] = ('hu' in name.lower())\n",
    "            # has(rv) = True\n",
    "            features[\"has(rv)\"] = ('rv' in name.lower())    \n",
    "            # has(rw) = True\n",
    "            features[\"has(rw)\"] = ('rw' in name.lower()) \n",
    "            # has(sp) = True\n",
    "            features[\"has(sp)\"] = ('sp' in name.lower())\n",
    "\n",
    "            # lastletter = 'a'\n",
    "            features[\"lastletter=a\"] = ('a' in name[-1:].lower())\n",
    "            # lastletter = 'f'\n",
    "            features[\"lastletter=f\"] = ('f' in name[-1:].lower())\n",
    "            # lastletter = 'k'\n",
    "            features[\"lastletter=k\"] = ('k' in name[-1:].lower())\n",
    "\n",
    "            # last2letter = 'ch'\n",
    "            features[\"last2letter=ch\"] = ('ch' in name[-2:].lower())\n",
    "            # last2letter = 'do'\n",
    "            features[\"last2letter=do\"] = ('do' in name[-2:].lower())\n",
    "            # last2letter = 'ia'\n",
    "            features[\"last2letter=ia\"] = ('ia' in name[-2:].lower())\n",
    "            # last2letter = 'im'\n",
    "            features[\"last2letter=im\"] = ('im' in name[-2:].lower())\n",
    "            # last2letter = 'io'\n",
    "            features[\"last2letter=io\"] = ('io' in name[-2:].lower())\n",
    "            # last2letter = 'la'\n",
    "            features[\"last2letter=la\"] = ('la' in name[-2:].lower())\n",
    "            # last2letter = 'ld'\n",
    "            features[\"last2letter=ld\"] = ('ld' in name[-2:].lower())\n",
    "            # last2letter = 'na'\n",
    "            features[\"last2letter=na\"] = ('na' in name[-2:].lower())\n",
    "            # last2letter = 'os'\n",
    "            features[\"last2letter=os\"] = ('os' in name[-2:].lower())\n",
    "            # last2letter = 'ra'\n",
    "            features[\"last2letter=ra\"] = ('ra' in name[-2:].lower())\n",
    "            # last2letter = 'rd'\n",
    "            features[\"last2letter=rd\"] = ('rd' in name[-2:].lower())\n",
    "            # last2letter = 'rt'\n",
    "            features[\"last2letter=rt\"] = ('rt' in name[-2:].lower())\n",
    "            # last2letter = 'sa'\n",
    "            features[\"last2letter=sa\"] = ('sa' in name[-2:].lower())\n",
    "            # last2letter = 'ta'\n",
    "            features[\"last2letter=ta\"] = ('ta' in name[-2:].lower())\n",
    "            # last2letter = 'us'\n",
    "            features[\"last2letter=us\"] = ('us' in name[-2:].lower())\n",
    "\n",
    "            # last3letter = 'ana'\n",
    "            features[\"last3letter=ana\"] = ('ana' in name[-3:].lower())    \n",
    "            # last3letter = u'ard'\n",
    "            features[\"last3letter=ard\"] = ('ard' in name[-3:].lower())        \n",
    "            # last3letter = u'ita'\n",
    "            features[\"last3letter=ita\"] = ('ita' in name[-3:].lower())    \n",
    "            # last3letter = u'nne'\n",
    "            features[\"last3letter=nne\"] = ('nne' in name[-3:].lower())    \n",
    "            # last3letter = u'tta'\n",
    "            features[\"last3letter=tta\"] = ('tta' in name[-3:].lower())    \n",
    "        \n",
    "        return features\n",
    "\n",
    "    \n",
    "    def show_errors(self, errors, n=None):\n",
    "        if n is not None: errors = errors[:n]  \n",
    "        print(\"list of first %s errors :\" %(n))\n",
    "        for (tag, guess, name) in sorted(errors): \n",
    "            print('correct=%-8s guess=%-8s name=%-30s' %(tag, guess, name))\n",
    "        print(\"\")\n",
    "        return None \n",
    "    \n",
    "    \n",
    "    def classifier_report(self,classifier,dataset,feat_num):\n",
    "        feat_num = int(feat_num)\n",
    "        dataset_predictions = [classifier.classify(self.get_features(n,feat_num))  for (n, g) in dataset]\n",
    "        dataset_gold = [g  for (n, g) in dataset]\n",
    "        cm=ConfusionMatrix(dataset_gold, dataset_predictions)\n",
    "        print(\"Confusion Matrix: \")\n",
    "        print(cm)\n",
    "        print(\"\")\n",
    "    \n",
    "    def fit_model(self,feat_num_start, feat_num):\n",
    "       \n",
    "        for i in np.arange(feat_num_start, feat_num+1):\n",
    "            feat_num =int(i)\n",
    "            errors = []\n",
    "            \n",
    "            # devtest-set and training set are constructed\n",
    "            random.shuffle(development_set_names)\n",
    "            devtest_names, train_names = development_set_names[0:500], development_set_names[500:]\n",
    "            \n",
    "            train_set = [(self.get_features(n,feat_num), g)  for (n, g) in train_names]\n",
    "            devtest_set = [(self.get_features(n,feat_num), g)  for (n, g) in devtest_names]\n",
    "            test_set = [(self.get_features(n,feat_num), g)  for (n, g) in test_names] \n",
    "            \n",
    "            classifier = self.model.train(train_set) \n",
    "            \n",
    "            # For errors list\n",
    "            for (name, tag) in devtest_names:\n",
    "                guess = classifier.classify(self.get_features(name,feat_num)) \n",
    "                if guess != tag: \n",
    "                    errors.append((tag, guess, name))    \n",
    "                    \n",
    "            #Print classifier report\n",
    "            self.classifier_report(classifier,train_names,i)\n",
    "            \n",
    "            # Print errors        \n",
    "            self.show_errors(errors, 5)\n",
    "            \n",
    "            # Print show_most_informative_features for NaiveBayes\n",
    "            # Print pseudocode for DecisionTree\n",
    "            if self.model ==nltk.NaiveBayesClassifier:\n",
    "                classifier.show_most_informative_features(5)                \n",
    "            elif self.model ==nltk.DecisionTreeClassifier:\n",
    "                print(classifier.pseudocode(depth=5))\n",
    "            \n",
    "            print(\"-----------------------------------------\")\n",
    "            print(\"Accuracy of model {} using feature {} : {}\".format(self.model,feat_num,nltk.classify.accuracy(classifier, devtest_set)))\n",
    "            print(\"=========================================\")\n",
    "            print(\"\")\n",
    "            \n",
    "    def get_sorted_feature_accuracies(self,feat_num_start, feat_num):\n",
    "        feature_accuracy = {}\n",
    "        for i in np.arange(feat_num_start, feat_num+1):\n",
    "            feat_num =int(i)\n",
    "            errors = []\n",
    "            \n",
    "            # devtest-set and training set are constructed\n",
    "            random.shuffle(development_set_names)\n",
    "            devtest_names, train_names = development_set_names[0:500], development_set_names[500:]\n",
    "            \n",
    "            train_set = [(self.get_features(n,feat_num), g)  for (n, g) in train_names]\n",
    "            devtest_set = [(self.get_features(n,feat_num), g)  for (n, g) in devtest_names]\n",
    "            test_set = [(self.get_features(n,feat_num), g)  for (n, g) in test_names] \n",
    "            \n",
    "            classifier = self.model.train(train_set) \n",
    "            \n",
    "            # For errors list\n",
    "            for (name, tag) in devtest_names:\n",
    "                guess = classifier.classify(self.get_features(name,feat_num)) \n",
    "                if guess != tag: \n",
    "                    errors.append((tag, guess, name))    \n",
    "                    \n",
    "            \n",
    "            feature_accuracy[feat_num] = nltk.classify.accuracy(classifier, devtest_set)\n",
    "        \n",
    "        \n",
    "        #sort for accuracy, and then reverse the array to return the array as most accurate to least accurate\n",
    "        sorted_by_accuracy = sorted(feature_accuracy.items(), key=operator.itemgetter(1))\n",
    "        return sorted_by_accuracy[::-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier we selected is the Naive Bayes classifier.  \n",
    "\n",
    "We will first use feature 1 - 14 (single features) and examine how the model performs for each. These results will provide us with the basis to derive more complexe features to refine model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf = NLP_Classifier(nltk.NaiveBayesClassifier)\n",
    "# specify initial and final feature\n",
    "ranked_features = clf.get_sorted_feature_accuracies(1, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Five Single Features with the Highest Accuracy\n",
      "---------------------------------------------------------------\n",
      "Feature: Last 3 letters                 Accuracy: 0.8     \n",
      "Feature: Last two letters               Accuracy: 0.77    \n",
      "Feature: Count of pair of letters in the alphabet Accuracy: 0.758   \n",
      "Feature: Last Letter                    Accuracy: 0.738   \n",
      "Feature: Character count                Accuracy: 0.706   \n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "features = {\n",
    "    1: \"Last Letter\",\n",
    "    2: \"First Letter\",\n",
    "    3: \"Vowels count\",\n",
    "    4: \"Hard consonants using general rules of c and g\",\n",
    "    5: \"Soft consonants using general rules of c and g\",\n",
    "    6: \"Syllable Count of names via textstat\",\n",
    "    7: \"Name length\",\n",
    "    8: \"Last two letters\",\n",
    "    9: \"Last 3 letters\",\n",
    "    10: \"Character count\",\n",
    "    11: \"Character present\",\n",
    "    12: \"Count of each letter\",\n",
    "    13: \"Count of pair of letters in the alphabet\",\n",
    "    14: \"First 2 letters\"\n",
    "}\n",
    "\n",
    "print(\"Top Five Single Features with the Highest Accuracy\")\n",
    "print(\"---------------------------------------------------------------\")\n",
    "for (feat_num, accuracy) in ranked_features[0:5]:\n",
    "    print('Feature: %-30s Accuracy: %-8s' %(features[feat_num], accuracy))\n",
    "\n",
    "print(\"---------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3563> 807 |\n",
      "  male |  857<1717>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Allys                         \n",
      "correct=female   guess=male     name=Linnet                        \n",
      "correct=male     guess=female   name=Ashley                        \n",
      "correct=male     guess=female   name=Lennie                        \n",
      "correct=male     guess=female   name=Timmie                        \n",
      "\n",
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     32.3 : 1.0\n",
      "             last_letter = 'k'              male : female =     29.8 : 1.0\n",
      "             last_letter = 'f'              male : female =     15.2 : 1.0\n",
      "             last_letter = 'v'              male : female =     11.2 : 1.0\n",
      "             last_letter = 'd'              male : female =      9.6 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 1 : 0.776\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4151> 204 |\n",
      "  male | 2247 <342>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Garwin                        \n",
      "correct=male     guess=female   name=Goober                        \n",
      "correct=male     guess=female   name=Janus                         \n",
      "correct=male     guess=female   name=Judd                          \n",
      "correct=male     guess=female   name=Orin                          \n",
      "\n",
      "Most Informative Features\n",
      "            first_letter = 'w'              male : female =      4.6 : 1.0\n",
      "            first_letter = 'q'              male : female =      2.7 : 1.0\n",
      "            first_letter = 'x'              male : female =      2.6 : 1.0\n",
      "            first_letter = 'u'              male : female =      2.5 : 1.0\n",
      "            first_letter = 'k'            female : male   =      2.3 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 2 : 0.65\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3873> 489 |\n",
      "  male | 2063 <519>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Bell                          \n",
      "correct=female   guess=male     name=Kristy                        \n",
      "correct=male     guess=female   name=Anatol                        \n",
      "correct=male     guess=female   name=Meredeth                      \n",
      "correct=male     guess=female   name=Paco                          \n",
      "\n",
      "Most Informative Features\n",
      "             vowel_count = 5              female : male   =      3.5 : 1.0\n",
      "             vowel_count = 4              female : male   =      2.5 : 1.0\n",
      "             vowel_count = 1                male : female =      1.8 : 1.0\n",
      "             vowel_count = 3              female : male   =      1.5 : 1.0\n",
      "             vowel_count = 2                male : female =      1.5 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 3 : 0.64\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4360>   6 |\n",
      "  male | 2566  <12>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Bryan                         \n",
      "correct=male     guess=female   name=Ishmael                       \n",
      "correct=male     guess=female   name=Josephus                      \n",
      "correct=male     guess=female   name=Markos                        \n",
      "correct=male     guess=female   name=Tiebold                       \n",
      "\n",
      "Most Informative Features\n",
      "             hard_consts = 1.0              male : female =      3.3 : 1.0\n",
      "             hard_consts = 0.5              male : female =      1.2 : 1.0\n",
      "             hard_consts = 0.0            female : male   =      1.0 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 4 : 0.626\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4374>   . |\n",
      "  male | 2570   <.>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Bartholomew                   \n",
      "correct=male     guess=female   name=Hamlet                        \n",
      "correct=male     guess=female   name=Horatius                      \n",
      "correct=male     guess=female   name=Kenny                         \n",
      "correct=male     guess=female   name=Parnell                       \n",
      "\n",
      "Most Informative Features\n",
      "             soft_consts = 1.0            female : male   =      1.5 : 1.0\n",
      "             soft_consts = 0.5            female : male   =      1.1 : 1.0\n",
      "             soft_consts = 0.0              male : female =      1.0 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 5 : 0.612\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4353>   . |\n",
      "  male | 2591   <.>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Corbin                        \n",
      "correct=male     guess=female   name=Joseph                        \n",
      "correct=male     guess=female   name=Morton                        \n",
      "correct=male     guess=female   name=Reube                         \n",
      "correct=male     guess=female   name=Stefano                       \n",
      "\n",
      "Most Informative Features\n",
      "          syllable_count = 3.6            female : male   =      3.3 : 1.0\n",
      "          syllable_count = 2.7            female : male   =      2.0 : 1.0\n",
      "          syllable_count = 0.9              male : female =      1.6 : 1.0\n",
      "          syllable_count = 1.8              male : female =      1.1 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 6 : 0.654\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4263> 119 |\n",
      "  male | 2425 <137>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Adams                         \n",
      "correct=male     guess=female   name=Cobb                          \n",
      "correct=male     guess=female   name=Erick                         \n",
      "correct=male     guess=female   name=Lionello                      \n",
      "correct=male     guess=female   name=Marcio                        \n",
      "\n",
      "Most Informative Features\n",
      "                  length = 2                male : female =      3.0 : 1.0\n",
      "                  length = 3                male : female =      1.9 : 1.0\n",
      "                  length = 12               male : female =      1.7 : 1.0\n",
      "                  length = 15               male : female =      1.7 : 1.0\n",
      "                  length = 13               male : female =      1.7 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 7 : 0.602\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3975> 413 |\n",
      "  male |  961<1595>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Flower                        \n",
      "correct=female   guess=male     name=Kristin                       \n",
      "correct=male     guess=female   name=Darryl                        \n",
      "correct=male     guess=female   name=Kit                           \n",
      "correct=male     guess=female   name=Thorny                        \n",
      "\n",
      "Most Informative Features\n",
      "            last2letters = 'na'           female : male   =     90.5 : 1.0\n",
      "            last2letters = 'la'           female : male   =     71.6 : 1.0\n",
      "            last2letters = 'ia'           female : male   =     37.0 : 1.0\n",
      "            last2letters = 'ld'             male : female =     36.4 : 1.0\n",
      "            last2letters = 'ra'           female : male   =     35.6 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 8 : 0.778\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4066> 292 |\n",
      "  male |  689<1897>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Ainsley                       \n",
      "correct=female   guess=male     name=Berty                         \n",
      "correct=female   guess=male     name=Harriot                       \n",
      "correct=male     guess=female   name=Paco                          \n",
      "correct=male     guess=female   name=Pip                           \n",
      "\n",
      "Most Informative Features\n",
      "            last3letters = 'nne'          female : male   =     30.9 : 1.0\n",
      "            last3letters = 'ana'          female : male   =     24.0 : 1.0\n",
      "            last3letters = 'tta'          female : male   =     23.1 : 1.0\n",
      "            last3letters = 'son'            male : female =     19.5 : 1.0\n",
      "            last3letters = 'ard'            male : female =     19.2 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 9 : 0.772\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3782> 604 |\n",
      "  male | 1418<1140>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Dorri                         \n",
      "correct=female   guess=male     name=Lulu                          \n",
      "correct=male     guess=female   name=Jeremias                      \n",
      "correct=male     guess=female   name=Joel                          \n",
      "correct=male     guess=female   name=Levi                          \n",
      "\n",
      "Most Informative Features\n",
      "                 count_v = 2              female : male   =      8.8 : 1.0\n",
      "                 count_w = 2                male : female =      5.1 : 1.0\n",
      "                 count_a = 3              female : male   =      4.8 : 1.0\n",
      "                 count_i = 3                male : female =      4.5 : 1.0\n",
      "                 count_w = 1                male : female =      4.4 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 10 : 0.684\n",
      "=========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3782> 586 |\n",
      "  male | 1540<1036>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Artie                         \n",
      "correct=male     guess=female   name=Austen                        \n",
      "correct=male     guess=female   name=Micheil                       \n",
      "correct=male     guess=female   name=Rene                          \n",
      "correct=male     guess=female   name=Salim                         \n",
      "\n",
      "Most Informative Features\n",
      "                   has_w = True             male : female =      4.2 : 1.0\n",
      "                   has_u = True             male : female =      1.8 : 1.0\n",
      "                   has_p = True             male : female =      1.8 : 1.0\n",
      "                   has_f = True             male : female =      1.7 : 1.0\n",
      "                   has_o = True             male : female =      1.6 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 11 : 0.716\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3732> 606 |\n",
      "  male | 1409<1197>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Eddy                          \n",
      "correct=female   guess=male     name=Peggy                         \n",
      "correct=female   guess=male     name=Shawna                        \n",
      "correct=male     guess=female   name=Allen                         \n",
      "correct=male     guess=female   name=Jimbo                         \n",
      "\n",
      "Most Informative Features\n",
      "                count(v) = 2              female : male   =      9.0 : 1.0\n",
      "                count(w) = 1                male : female =      4.3 : 1.0\n",
      "                count(a) = 3              female : male   =      4.0 : 1.0\n",
      "                count(o) = 2                male : female =      3.7 : 1.0\n",
      "                count(a) = 2              female : male   =      3.2 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 12 : 0.712\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3719> 654 |\n",
      "  male | 1023<1548>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Brooke                        \n",
      "correct=male     guess=female   name=Alejandro                     \n",
      "correct=male     guess=female   name=Chris                         \n",
      "correct=male     guess=female   name=Kam                           \n",
      "correct=male     guess=female   name=Ransell                       \n",
      "\n",
      "Most Informative Features\n",
      "                 has(hu) = True             male : female =     36.8 : 1.0\n",
      "                 has(rv) = True             male : female =     30.0 : 1.0\n",
      "                 has(fo) = True             male : female =     19.4 : 1.0\n",
      "                 has(sp) = True             male : female =     16.4 : 1.0\n",
      "                 has(lt) = True             male : female =     15.3 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 13 : 0.762\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3904> 445 |\n",
      "  male | 1740 <855>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Ranique                       \n",
      "correct=female   guess=male     name=Stacey                        \n",
      "correct=male     guess=female   name=Leroy                         \n",
      "correct=male     guess=female   name=Marcos                        \n",
      "correct=male     guess=female   name=Truman                        \n",
      "\n",
      "Most Informative Features\n",
      "           first2Letters = 'hu'             male : female =     17.0 : 1.0\n",
      "           first2Letters = 'ya'             male : female =     11.5 : 1.0\n",
      "           first2Letters = 'tu'             male : female =     10.4 : 1.0\n",
      "           first2Letters = 'wa'             male : female =      9.1 : 1.0\n",
      "           first2Letters = 'wh'             male : female =      8.2 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 14 : 0.7\n",
      "=========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit_model(1, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of single features for Naive Baysien Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on results for features 1-14, we can see that the best performing features are; 1, 8, 9, 10, 11, 13. Combining these features, we produced features 15 & 16.  \n",
    "\n",
    "We will now evaluate classifier with these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3732> 623 |\n",
      "  male |  503<2086>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Charmain                      \n",
      "correct=female   guess=male     name=Chickie                       \n",
      "correct=female   guess=male     name=Cloris                        \n",
      "correct=female   guess=male     name=Rianon                        \n",
      "correct=female   guess=male     name=Veronique                     \n",
      "\n",
      "Most Informative Features\n",
      "             last2letter = 'na'           female : male   =     95.5 : 1.0\n",
      "             last2letter = 'la'           female : male   =     72.2 : 1.0\n",
      "             last2letter = 'ra'           female : male   =     58.8 : 1.0\n",
      "                 has(hu) = True             male : female =     37.6 : 1.0\n",
      "             last2letter = 'ia'           female : male   =     37.0 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 15 : 0.852\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<1631>2732 |\n",
      "  male |   25<2556>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Alanah                        \n",
      "correct=female   guess=male     name=Annamarie                     \n",
      "correct=female   guess=male     name=Fanchette                     \n",
      "correct=female   guess=male     name=Jo Ann                        \n",
      "correct=male     guess=female   name=Shea                          \n",
      "\n",
      "Most Informative Features\n",
      "          last2letter=na = True           female : male   =     93.6 : 1.0\n",
      "          last2letter=la = True           female : male   =     66.5 : 1.0\n",
      "          last2letter=ia = True           female : male   =     36.4 : 1.0\n",
      "            lastletter=a = True           female : male   =     34.8 : 1.0\n",
      "          last2letter=ra = True           female : male   =     33.7 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 16 : 0.59\n",
      "=========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit_model(15, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, we would consider feature 15 to build the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Decision Tree\n",
    "\n",
    "The second classifier we selected is the Decision Tree classifier.  \n",
    "\n",
    "Again, we will first use features 1 - 14 (single features) and examine how the model perform for each. These results will provide us with the basis to derive more complexe features to refine model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3590> 802 |\n",
      "  male |  840<1712>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Clo                           \n",
      "correct=female   guess=male     name=Dell                          \n",
      "correct=female   guess=male     name=Row                           \n",
      "correct=male     guess=female   name=Connie                        \n",
      "correct=male     guess=female   name=Godfree                       \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 1 : 0.748\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4170> 206 |\n",
      "  male | 2218 <350>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Quentin                       \n",
      "correct=male     guess=female   name=Forster                       \n",
      "correct=male     guess=female   name=Joachim                       \n",
      "correct=male     guess=female   name=Lucas                         \n",
      "correct=male     guess=female   name=Owen                          \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 2 : 0.64\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3882> 501 |\n",
      "  male | 2052 <509>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Tandy                         \n",
      "correct=male     guess=female   name=Arnie                         \n",
      "correct=male     guess=female   name=Bartholomeus                  \n",
      "correct=male     guess=female   name=Fraser                        \n",
      "correct=male     guess=female   name=Martainn                      \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 3 : 0.628\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4362>   6 |\n",
      "  male | 2565  <11>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Al                            \n",
      "correct=male     guess=female   name=Antony                        \n",
      "correct=male     guess=female   name=Jae                           \n",
      "correct=male     guess=female   name=Laurance                      \n",
      "correct=male     guess=female   name=Vlad                          \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 4 : 0.658\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4398>   . |\n",
      "  male | 2546   <.>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Harlin                        \n",
      "correct=male     guess=female   name=Llewellyn                     \n",
      "correct=male     guess=female   name=Partha                        \n",
      "correct=male     guess=female   name=Reinhold                      \n",
      "correct=male     guess=female   name=Siddhartha                    \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 5 : 0.598\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4369>   . |\n",
      "  male | 2575   <.>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Antonin                       \n",
      "correct=male     guess=female   name=Hasty                         \n",
      "correct=male     guess=female   name=Heath                         \n",
      "correct=male     guess=female   name=Pablo                         \n",
      "correct=male     guess=female   name=Wyatan                        \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 6 : 0.656\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4262> 120 |\n",
      "  male | 2419 <143>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Bev                           \n",
      "correct=male     guess=female   name=Gearard                       \n",
      "correct=male     guess=female   name=Jeffry                        \n",
      "correct=male     guess=female   name=Kingston                      \n",
      "correct=male     guess=female   name=Magnum                        \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 7 : 0.624\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3919> 486 |\n",
      "  male |  874<1665>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Avery                         \n",
      "correct=male     guess=female   name=Ely                           \n",
      "correct=male     guess=female   name=Enrique                       \n",
      "correct=male     guess=female   name=Hillery                       \n",
      "correct=male     guess=female   name=Marlowe                       \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 8 : 0.808\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3903> 471 |\n",
      "  male |  512<2058>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Kary                          \n",
      "correct=male     guess=female   name=Allyn                         \n",
      "correct=male     guess=female   name=Hassan                        \n",
      "correct=male     guess=female   name=Murphy                        \n",
      "correct=male     guess=female   name=Shay                          \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 9 : 0.78\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4138> 229 |\n",
      "  male | 1163<1414>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Drew                          \n",
      "correct=male     guess=female   name=Arie                          \n",
      "correct=male     guess=female   name=Enoch                         \n",
      "correct=male     guess=female   name=Ernest                        \n",
      "correct=male     guess=female   name=Noland                        \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 10 : 0.736\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4317>  69 |\n",
      "  male | 2333 <225>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Baillie                       \n",
      "correct=male     guess=female   name=Linoel                        \n",
      "correct=male     guess=female   name=Merrick                       \n",
      "correct=male     guess=female   name=Petey                         \n",
      "correct=male     guess=female   name=Tarzan                        \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 11 : 0.642\n",
      "=========================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4150> 219 |\n",
      "  male | 1175<1400>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Agretha                       \n",
      "correct=female   guess=male     name=Felipa                        \n",
      "correct=male     guess=female   name=Barri                         \n",
      "correct=male     guess=female   name=Lex                           \n",
      "correct=male     guess=female   name=Reinhold                      \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 12 : 0.716\n",
      "=========================================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1e3f9ebd886e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclf_dt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNLP_Classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# specify initial and final feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf_dt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-9e1331c2398f>\u001b[0m in \u001b[0;36mfit_model\u001b[1;34m(self, feat_num_start, feat_num)\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeat_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m             \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[1;31m# For errors list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             tree = DecisionTreeClassifier.best_stump(\n\u001b[1;32m--> 154\u001b[1;33m                 feature_names, labeled_featuresets, verbose)\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             tree = DecisionTreeClassifier.best_binary_stump(\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mbest_stump\u001b[1;34m(feature_names, labeled_featuresets, verbose)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[0mstump\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[0mstump_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstump\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstump_error\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0mbest_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstump_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, labeled_featuresets)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m# Decision tree:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mfval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decisions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decisions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf_dt = NLP_Classifier(nltk.DecisionTreeClassifier)\n",
    "# specify initial and final feature\n",
    "clf_dt.fit_model(1, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, it would seem that features; 1, 8, 9, 10, 12 lead to better outcomes.  We will now evaluate the evaluate the combination features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3748> 615 |\n",
      "  male |  507<2074>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Agnes                         \n",
      "correct=female   guess=male     name=Hedy                          \n",
      "correct=female   guess=male     name=Ivory                         \n",
      "correct=female   guess=male     name=Kirstin                       \n",
      "correct=male     guess=female   name=Kenneth                       \n",
      "\n",
      "Most Informative Features\n",
      "             last2letter = 'na'           female : male   =     92.8 : 1.0\n",
      "             last2letter = 'la'           female : male   =     70.6 : 1.0\n",
      "             last2letter = 'ia'           female : male   =     37.5 : 1.0\n",
      "                 has(hu) = True             male : female =     36.6 : 1.0\n",
      "             last2letter = 'ra'           female : male   =     35.1 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 15 : 0.82\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<1628>2737 |\n",
      "  male |   27<2552>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Joline                        \n",
      "correct=female   guess=male     name=Korrie                        \n",
      "correct=female   guess=male     name=Revkah                        \n",
      "correct=female   guess=male     name=Stephannie                    \n",
      "correct=female   guess=male     name=Tove                          \n",
      "\n",
      "Most Informative Features\n",
      "          last2letter=na = True           female : male   =     89.9 : 1.0\n",
      "          last2letter=la = True           female : male   =     68.0 : 1.0\n",
      "          last2letter=ia = True           female : male   =     37.6 : 1.0\n",
      "                 has(hu) = True             male : female =     35.5 : 1.0\n",
      "            lastletter=a = True           female : male   =     34.7 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 16 : 0.604\n",
      "=========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit_model(15, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection (Choose Best Candidate) \n",
    "\n",
    "Now that we have the best model build with both classifiers, we will test the model against the test data set and compare the results.  \n",
    "\n",
    "\n",
    "#### Check the model's final performance on the test set. \n",
    "\n",
    "#### How does the performance on the test set compare to the performance on the dev-test set? \n",
    "\n",
    "#### Is this what you'd expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3\n",
    "\n",
    "This is a collaborative project conducted by the Fall 2017 students of DATA 620 at The City University of New York, in partial fulfillment of the requirements for the MS in Data Science degree.\n",
    "\n",
    "### Problem Description\n",
    "\n",
    "This is a Team Project! For this project, please work with the entire class as one collaborative group! Your project should be submitted (as an IPython Notebook via GitHub) by end of day on Monday, October 25th. The group should present their code and findings in our meet-up on Tuesday October 26th. The ability to be an effective member of a virtual team is highly valued in the data science job market.\n",
    "Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can. Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6900 words for the training set. Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set. How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?\n",
    "\n",
    "Source: Natural Language Processing with Python, exercise 6.10.2.\n",
    "\n",
    "### Contributors Include\n",
    "\n",
    "* K. Joy Payton\n",
    "* Keith Folsom\n",
    "* Sonya Hong\n",
    "* Shyam Balagurumurthy Viswanathan\n",
    "* Derek Nokes\n",
    "* Liam Byrne\n",
    "* Latif Masud\n",
    "* Valerie Briot\n",
    "\n",
    "\n",
    "### First, Obtain the Corpus\n",
    "\n",
    "Note: If not already executed, nltk.download() will allow you access to the names corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing required libraries/packages\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.metrics import *\n",
    "import re\n",
    "    \n",
    "\n",
    "import string\n",
    "from textstat.textstat import textstat\n",
    "#nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ([(name, 'male') for name in names.words('male.txt')] + \\\n",
    "         [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "#names = random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Jeniffer', 'female'),\n",
       " (u'Laurie', 'male'),\n",
       " (u'Letta', 'female'),\n",
       " (u'Letty', 'female'),\n",
       " (u'Jasmina', 'female'),\n",
       " (u'Joya', 'female'),\n",
       " (u'Fonsie', 'male'),\n",
       " (u'Major', 'male'),\n",
       " (u'Joann', 'female')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(names)\n",
    "\n",
    "# let's see what the randomly shuffles names look like\n",
    "names[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create three subsets for development and error analysis of the models.\n",
    "\n",
    "##### Development set:\n",
    "* 6900 names for the training set\n",
    "* 500 names for the dev-test set  \n",
    "\n",
    "##### Test set:\n",
    "* 500 names for the testing set\n",
    "\n",
    "We will first split the \"names\" data set into a Development set of 7,400 entries and a Test set of 500 entries.  \n",
    "The Development set will be used to test each features as we build the module. This test will be split between a training set and a dev-test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the names set into a Development set and a Test set;\n",
    "# Developmeent set will be used for training and testing each features as we build the model\n",
    "#test_names, devtest_names, train_names = names[0:500], names[500:1000], names[1000:]\n",
    "test_names, development_set_names = names[0:500], names[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development Set = 7444\n",
      "Test Set = 500\n"
     ]
    }
   ],
   "source": [
    "# Confirm the size of the three subsets\n",
    "#print(\"Training Set = {}\".format(len(train_names)))\n",
    "#print(\"Dev-Test Set = {}\".format(len(devtest_names)))\n",
    "print(\"Development Set = {}\".format(len(development_set_names)))\n",
    "print(\"Test Set = {}\".format(len(test_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor Functions\n",
    "\n",
    "This section below is to incrementallly improve the feature extraction functions which are subsequently applied to the development and test datasets.\n",
    "\n",
    "We will make use of various documentation to identify various features; \n",
    "\n",
    "1. Last Letter\n",
    "2. First Letter, most names begining with a vowel are associated with females  \n",
    "3. Vowels count\n",
    "4. Hard consonants using general rules of c and g\n",
    "5. Soft consonants using general rules of c and g\n",
    "6. Syllable Count of names via textstat \n",
    "7. Name length\n",
    "8. Last two letters\n",
    "9. Last 3 letters\n",
    "10. Character count\n",
    "11. Character present\n",
    "12. Letter count??? --> need to check with Derek?\n",
    "13. Letter pair present?? --> need to check with Derek?\n",
    "14. First 2 letters\n",
    "15. First letter, Last Letter, Last 2 Letters, last 3 Letters, 2-grams\n",
    "16. ???\n",
    "\n",
    "The features are coded below.  \n",
    "\n",
    "We will use NaiveBayes and Decision tree classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Identification Models building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate the analysis, a class for classifier was constructed. This class will have all the features and various methods; show error, display confusion matrix, evaluate fit of model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP_Classifier():\n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "        #self.feat_num = feat_num\n",
    "        \n",
    "        #train_set = [(gender_features3(n), g)  for (n, g) in train_names]\n",
    "        #devtest_set = [(gender_features3(n), g)  for (n, g) in devtest_names]\n",
    "        #test_set = [(gender_features3(n), g)  for (n, g) in test_names]\n",
    "\n",
    "\n",
    "    def get_features(self,name,feat_num):\n",
    "        '''\n",
    "        Parameters:\n",
    "            name - string of name to extract feature\n",
    "            feat_num - itterable collection of integers specifying features. *Defaults to 1:9 inclusive\n",
    "                1: last letter\n",
    "                2: first letter\n",
    "                3: Vowel counts\n",
    "                4: Hard consonant count\n",
    "                5: Soft consonant count\n",
    "                6: Syllable Count\n",
    "                7: Name length\n",
    "                8: Last two chars\n",
    "                9: Last three chars\n",
    "                10: char count --> feature for all alpha chars\n",
    "                11: char present --> feature for all alpha chars (boolean)\n",
    "        Returns:\n",
    "            features: a dictionary of extracted features\n",
    "        '''\n",
    "        features = {}    \n",
    "        \n",
    "        # Converts feat_num to itterable if type is int\n",
    "        if type(feat_num) is int:\n",
    "            feat_num = (0, feat_num)        \n",
    "       \n",
    "        # Gender Feature 1: Last letter - book example\n",
    "        if 1 in feat_num:\n",
    "            features['last_letter'] = name[-1].lower()\n",
    "            \n",
    "        # Gender Feature 2: First letter - most names beginning with a vowel --> females\n",
    "        if 2 in feat_num:\n",
    "            features['first_letter'] = name[0].lower()\n",
    "            \n",
    "        # Gender Feature 3: Vowel Counts\n",
    "        if 3 in feat_num:\n",
    "            features['vowel_count'] = len(re.sub(r'[^aeiou]', '', name.lower()))\n",
    "            \n",
    "        # Gender Feature 4: Hard consonants using general rules of c and g\n",
    "        if 4 in feat_num:\n",
    "            features['hard_consts'] = len(re.findall(r'[cg][^eiy]', name.lower()))/2\n",
    "            \n",
    "        # Gender Feature 5: Soft consonants using general rules of c and g\n",
    "        if 5 in feat_num:\n",
    "            features['soft_consts'] = len(re.findall(r'[cg][eiy]', name.lower()))/2\n",
    "            \n",
    "        # Gender Feature 6: Syllable Count of names via textstat\n",
    "        if 6 in feat_num:\n",
    "            features['syllable_count'] = textstat.syllable_count(name.lower())\n",
    "    \n",
    "        # Gender Feature 7: Name length\n",
    "        if 7 in feat_num:\n",
    "            features[\"length\"] = len(name)\n",
    "        \n",
    "        # Gender Feature 8: Last two chars\n",
    "        if 8 in feat_num:\n",
    "            features[\"last2letters\"] = name[-2:].lower()\n",
    "            \n",
    "        # Gender Feature 9: Last three chars\n",
    "        if 9 in feat_num:\n",
    "            features[\"last3letters\"] = name[-3:].lower()\n",
    "    \n",
    "        # Gender Feature 10: Char Counts (overfitts)\n",
    "        if 10 in feat_num:\n",
    "            for letter in string.ascii_lowercase:\n",
    "                features[\"count_{0}\".format(letter)] = name.lower().count(letter)\n",
    "                \n",
    "        # Gender Feature 11: Char Booleans (overfitts)\n",
    "        if 11 in feat_num:\n",
    "            for letter in string.ascii_lowercase:\n",
    "                features[\"has_{0}\".format(letter)] = letter in name.lower()\n",
    "        \n",
    "        \n",
    "        if 12 in feat_num:\n",
    "            features = {}\n",
    "            letters=list(map(chr, range(ord('a'), ord('z') + 1)))\n",
    "            for letter in letters:\n",
    "                features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "\n",
    "\n",
    "        if 13 in feat_num:\n",
    "            features = {}\n",
    "            letters=list(map(chr, range(ord('a'), ord('z') + 1)))\n",
    "            for letter1 in letters:\n",
    "                for letter2 in letters:\n",
    "                    features[\"has(\"+letter1+letter2+\")\"] = (letter1+letter2 in name.lower())\n",
    "\n",
    "        if 14 in feat_num:\n",
    "            features[\"first2Letters\"]=name[0:2].lower()\n",
    "\n",
    "        if 15 in feat_num:\n",
    "            features = {}\n",
    "            features[\"firstletter\"] = name[0].lower()\n",
    "            features[\"lastletter\"] = name[-1].lower()\n",
    "            features[\"last2letter\"] = name[-2:].lower()\n",
    "            features[\"last3letter\"] = name[-3:].lower()\n",
    "\n",
    "            letters=list(map(chr, range(ord('a'), ord('z') + 1)))\n",
    "            for letter1 in letters:\n",
    "                features[\"count(\"+letter1+\")\"] = name.lower().count(letter1)\n",
    "                features[\"has(\"+letter1+\")\"] = (letter1 in name.lower())\n",
    "                # iterate over 2-grams\n",
    "                for letter2 in letters:\n",
    "\n",
    "                    features[\"has(\"+letter1+letter2+\")\"] = (letter1+letter2 in name.lower())\n",
    "\n",
    "\n",
    "        if 16 in feat_num:\n",
    "            # define features\n",
    "            features = {}\n",
    "            # has(fo) = True\n",
    "            features[\"has(fo)\"] = ('fo' in name.lower())\n",
    "            # has(hu) = True\n",
    "            features[\"has(hu)\"] = ('hu' in name.lower())\n",
    "            # has(rv) = True\n",
    "            features[\"has(rv)\"] = ('rv' in name.lower())    \n",
    "            # has(rw) = True\n",
    "            features[\"has(rw)\"] = ('rw' in name.lower()) \n",
    "            # has(sp) = True\n",
    "            features[\"has(sp)\"] = ('sp' in name.lower())\n",
    "\n",
    "            # lastletter = 'a'\n",
    "            features[\"lastletter=a\"] = ('a' in name[-1:].lower())\n",
    "            # lastletter = 'f'\n",
    "            features[\"lastletter=f\"] = ('f' in name[-1:].lower())\n",
    "            # lastletter = 'k'\n",
    "            features[\"lastletter=k\"] = ('k' in name[-1:].lower())\n",
    "\n",
    "            # last2letter = 'ch'\n",
    "            features[\"last2letter=ch\"] = ('ch' in name[-2:].lower())\n",
    "            # last2letter = 'do'\n",
    "            features[\"last2letter=do\"] = ('do' in name[-2:].lower())\n",
    "            # last2letter = 'ia'\n",
    "            features[\"last2letter=ia\"] = ('ia' in name[-2:].lower())\n",
    "            # last2letter = 'im'\n",
    "            features[\"last2letter=im\"] = ('im' in name[-2:].lower())\n",
    "            # last2letter = 'io'\n",
    "            features[\"last2letter=io\"] = ('io' in name[-2:].lower())\n",
    "            # last2letter = 'la'\n",
    "            features[\"last2letter=la\"] = ('la' in name[-2:].lower())\n",
    "            # last2letter = 'ld'\n",
    "            features[\"last2letter=ld\"] = ('ld' in name[-2:].lower())\n",
    "            # last2letter = 'na'\n",
    "            features[\"last2letter=na\"] = ('na' in name[-2:].lower())\n",
    "            # last2letter = 'os'\n",
    "            features[\"last2letter=os\"] = ('os' in name[-2:].lower())\n",
    "            # last2letter = 'ra'\n",
    "            features[\"last2letter=ra\"] = ('ra' in name[-2:].lower())\n",
    "            # last2letter = 'rd'\n",
    "            features[\"last2letter=rd\"] = ('rd' in name[-2:].lower())\n",
    "            # last2letter = 'rt'\n",
    "            features[\"last2letter=rt\"] = ('rt' in name[-2:].lower())\n",
    "            # last2letter = 'sa'\n",
    "            features[\"last2letter=sa\"] = ('sa' in name[-2:].lower())\n",
    "            # last2letter = 'ta'\n",
    "            features[\"last2letter=ta\"] = ('ta' in name[-2:].lower())\n",
    "            # last2letter = 'us'\n",
    "            features[\"last2letter=us\"] = ('us' in name[-2:].lower())\n",
    "\n",
    "            # last3letter = 'ana'\n",
    "            features[\"last3letter=ana\"] = ('ana' in name[-3:].lower())    \n",
    "            # last3letter = u'ard'\n",
    "            features[\"last3letter=ard\"] = ('ard' in name[-3:].lower())        \n",
    "            # last3letter = u'ita'\n",
    "            features[\"last3letter=ita\"] = ('ita' in name[-3:].lower())    \n",
    "            # last3letter = u'nne'\n",
    "            features[\"last3letter=nne\"] = ('nne' in name[-3:].lower())    \n",
    "            # last3letter = u'tta'\n",
    "            features[\"last3letter=tta\"] = ('tta' in name[-3:].lower())    \n",
    "        \n",
    "        return features\n",
    "\n",
    "    \n",
    "    def show_errors(self, errors, n=None):\n",
    "        if n is not None: errors = errors[:n]  \n",
    "        print(\"list of first %s errors :\" %(n))\n",
    "        for (tag, guess, name) in sorted(errors): \n",
    "            print('correct=%-8s guess=%-8s name=%-30s' %(tag, guess, name))\n",
    "        print(\"\")\n",
    "        return None \n",
    "    \n",
    "    \n",
    "    def classifier_report(self,classifier,dataset,feat_num):\n",
    "        feat_num = int(feat_num)\n",
    "        dataset_predictions = [classifier.classify(self.get_features(n,feat_num))  for (n, g) in dataset]\n",
    "        dataset_gold = [g  for (n, g) in dataset]\n",
    "        cm=ConfusionMatrix(dataset_gold, dataset_predictions)\n",
    "        print(\"Confusion Matrix: \")\n",
    "        print(cm)\n",
    "        print(\"\")\n",
    "    \n",
    "    def fit_model(self,feat_num_start, feat_num):\n",
    "       \n",
    "        for i in np.arange(feat_num_start, feat_num+1):\n",
    "            feat_num =int(i)\n",
    "            errors = []\n",
    "            \n",
    "            # devtest-set and training set are constructed\n",
    "            random.shuffle(development_set_names)\n",
    "            devtest_names, train_names = development_set_names[0:500], development_set_names[500:]\n",
    "            \n",
    "            train_set = [(self.get_features(n,feat_num), g)  for (n, g) in train_names]\n",
    "            devtest_set = [(self.get_features(n,feat_num), g)  for (n, g) in devtest_names]\n",
    "            test_set = [(self.get_features(n,feat_num), g)  for (n, g) in test_names] \n",
    "            \n",
    "            classifier = self.model.train(train_set) \n",
    "            \n",
    "            # For errors list\n",
    "            for (name, tag) in devtest_names:\n",
    "                guess = classifier.classify(self.get_features(name,feat_num)) \n",
    "                if guess != tag: \n",
    "                    errors.append((tag, guess, name))    \n",
    "                    \n",
    "            #Print classifier report\n",
    "            self.classifier_report(classifier,train_names,i)\n",
    "            \n",
    "            # Print errors        \n",
    "            self.show_errors(errors, 5)\n",
    "            \n",
    "            # Print show_most_informative_features for NaiveBayes\n",
    "            # Print pseudocode for DecisionTree\n",
    "            if self.model ==nltk.NaiveBayesClassifier:\n",
    "                classifier.show_most_informative_features(5)                \n",
    "            elif self.model ==nltk.DecisionTreeClassifier:\n",
    "                print(classifier.pseudocode(depth=5))\n",
    "            \n",
    "            print(\"-----------------------------------------\")\n",
    "            print(\"Accuracy of model {} using feature {} : {}\".format(self.model,feat_num,nltk.classify.accuracy(classifier, devtest_set)))\n",
    "            print(\"=========================================\")\n",
    "            print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  NaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier we selected is the Naive Bayes classifier.  \n",
    "\n",
    "We will first use feature 1 - 14 (single features) and examine how the model perform for each. These results will provide us with the basis to derive more complexe features to refine model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3579> 798 |\n",
      "  male |  851<1716>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Gwendolen                     \n",
      "correct=female   guess=male     name=Storm                         \n",
      "correct=male     guess=female   name=Arne                          \n",
      "correct=male     guess=female   name=Bruce                         \n",
      "correct=male     guess=female   name=Randi                         \n",
      "\n",
      "Most Informative Features\n",
      "             last_letter = u'k'             male : female =     43.9 : 1.0\n",
      "             last_letter = u'a'           female : male   =     37.1 : 1.0\n",
      "             last_letter = u'f'             male : female =     13.3 : 1.0\n",
      "             last_letter = u'p'             male : female =     12.6 : 1.0\n",
      "             last_letter = u'm'             male : female =     10.3 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 1 : 0.762\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4200> 175 |\n",
      "  male | 2250 <319>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Chet                          \n",
      "correct=male     guess=female   name=Emmett                        \n",
      "correct=male     guess=female   name=Fletch                        \n",
      "correct=male     guess=female   name=Jasper                        \n",
      "correct=male     guess=female   name=Sinclair                      \n",
      "\n",
      "Most Informative Features\n",
      "            first_letter = u'w'             male : female =      4.7 : 1.0\n",
      "            first_letter = u'q'             male : female =      3.1 : 1.0\n",
      "            first_letter = u'u'             male : female =      2.7 : 1.0\n",
      "            first_letter = u'h'             male : female =      2.4 : 1.0\n",
      "            first_letter = u'k'           female : male   =      2.4 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 2 : 0.64\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3871> 496 |\n",
      "  male | 2063 <514>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Glynnis                       \n",
      "correct=male     guess=female   name=Carlie                        \n",
      "correct=male     guess=female   name=Dane                          \n",
      "correct=male     guess=female   name=Griffin                       \n",
      "correct=male     guess=female   name=Trace                         \n",
      "\n",
      "Most Informative Features\n",
      "             vowel_count = 5              female : male   =      3.3 : 1.0\n",
      "             vowel_count = 4              female : male   =      2.6 : 1.0\n",
      "             vowel_count = 1                male : female =      1.8 : 1.0\n",
      "             vowel_count = 0                male : female =      1.6 : 1.0\n",
      "             vowel_count = 3              female : male   =      1.5 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 3 : 0.64\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4376>   7 |\n",
      "  male | 2550  <11>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Cristopher                    \n",
      "correct=male     guess=female   name=Ezra                          \n",
      "correct=male     guess=female   name=Hewet                         \n",
      "correct=male     guess=female   name=Stearn                        \n",
      "correct=male     guess=female   name=Thaddius                      \n",
      "\n",
      "Most Informative Features\n",
      "             hard_consts = 1                male : female =      2.6 : 1.0\n",
      "             hard_consts = 0              female : male   =      1.0 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 4 : 0.63\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4395>   . |\n",
      "  male | 2549   <.>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Henrique                      \n",
      "correct=male     guess=female   name=Romain                        \n",
      "correct=male     guess=female   name=Romeo                         \n",
      "correct=male     guess=female   name=Simeon                        \n",
      "correct=male     guess=female   name=Tucker                        \n",
      "\n",
      "Most Informative Features\n",
      "             soft_consts = 1              female : male   =      1.8 : 1.0\n",
      "             soft_consts = 0                male : female =      1.0 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 5 : 0.604\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4374>   . |\n",
      "  male | 2570   <.>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Aleck                         \n",
      "correct=male     guess=female   name=King                          \n",
      "correct=male     guess=female   name=Matias                        \n",
      "correct=male     guess=female   name=Rolando                       \n",
      "correct=male     guess=female   name=Syd                           \n",
      "\n",
      "Most Informative Features\n",
      "          syllable_count = 3.6            female : male   =      3.1 : 1.0\n",
      "          syllable_count = 2.7            female : male   =      2.1 : 1.0\n",
      "          syllable_count = 0.9              male : female =      1.6 : 1.0\n",
      "          syllable_count = 1.8              male : female =      1.1 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 6 : 0.646\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4257> 116 |\n",
      "  male | 2434 <137>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Mateo                         \n",
      "correct=male     guess=female   name=Phil                          \n",
      "correct=male     guess=female   name=Tamas                         \n",
      "correct=male     guess=female   name=Trip                          \n",
      "correct=male     guess=female   name=Zelig                         \n",
      "\n",
      "Most Informative Features\n",
      "                  length = 2                male : female =      2.6 : 1.0\n",
      "                  length = 3                male : female =      2.0 : 1.0\n",
      "                  length = 15               male : female =      1.7 : 1.0\n",
      "                  length = 10             female : male   =      1.4 : 1.0\n",
      "                  length = 12               male : female =      1.3 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 7 : 0.646\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3985> 420 |\n",
      "  male |  933<1606>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Ismail                        \n",
      "correct=male     guess=female   name=Krishna                       \n",
      "correct=male     guess=female   name=Pail                          \n",
      "correct=male     guess=female   name=Regen                         \n",
      "correct=male     guess=female   name=Willie                        \n",
      "\n",
      "Most Informative Features\n",
      "            last2letters = u'na'          female : male   =    153.7 : 1.0\n",
      "            last2letters = u'ia'          female : male   =     52.4 : 1.0\n",
      "            last2letters = u'sa'          female : male   =     34.7 : 1.0\n",
      "            last2letters = u'ra'          female : male   =     34.0 : 1.0\n",
      "            last2letters = u'us'            male : female =     28.9 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 8 : 0.774\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4086> 313 |\n",
      "  male |  674<1871>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Abigael                       \n",
      "correct=female   guess=male     name=Fred                          \n",
      "correct=female   guess=male     name=Quinn                         \n",
      "correct=male     guess=female   name=Benny                         \n",
      "correct=male     guess=female   name=Seth                          \n",
      "\n",
      "Most Informative Features\n",
      "            last3letters = u'ana'         female : male   =     22.2 : 1.0\n",
      "            last3letters = u'tta'         female : male   =     21.3 : 1.0\n",
      "            last3letters = u'ard'           male : female =     20.5 : 1.0\n",
      "            last3letters = u'nne'         female : male   =     19.4 : 1.0\n",
      "            last3letters = u'old'           male : female =     18.4 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 9 : 0.796\n",
      "=========================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3808> 589 |\n",
      "  male | 1412<1135>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Ashley                        \n",
      "correct=male     guess=female   name=Etienne                       \n",
      "correct=male     guess=female   name=Lynn                          \n",
      "correct=male     guess=female   name=Raj                           \n",
      "correct=male     guess=female   name=Zelig                         \n",
      "\n",
      "Most Informative Features\n",
      "                 count_v = 2              female : male   =      9.5 : 1.0\n",
      "                 count_w = 2                male : female =      5.2 : 1.0\n",
      "                 count_a = 3              female : male   =      4.9 : 1.0\n",
      "                 count_w = 1                male : female =      4.4 : 1.0\n",
      "                 count_o = 2                male : female =      3.5 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 10 : 0.692\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3809> 570 |\n",
      "  male | 1568 <997>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Chloe                         \n",
      "correct=female   guess=male     name=Jobye                         \n",
      "correct=male     guess=female   name=Nicolas                       \n",
      "correct=male     guess=female   name=Roland                        \n",
      "correct=male     guess=female   name=Tybalt                        \n",
      "\n",
      "Most Informative Features\n",
      "                   has_w = True             male : female =      4.3 : 1.0\n",
      "                   has_f = True             male : female =      1.8 : 1.0\n",
      "                   has_u = True             male : female =      1.8 : 1.0\n",
      "                   has_p = True             male : female =      1.7 : 1.0\n",
      "                   has_o = True             male : female =      1.5 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 11 : 0.72\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3788> 597 |\n",
      "  male | 1425<1134>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Evy                           \n",
      "correct=male     guess=female   name=Carleigh                      \n",
      "correct=male     guess=female   name=Hendrick                      \n",
      "correct=male     guess=female   name=Johannes                      \n",
      "correct=male     guess=female   name=Lind                          \n",
      "\n",
      "Most Informative Features\n",
      "                count(v) = 2              female : male   =      8.4 : 1.0\n",
      "                count(a) = 3              female : male   =      4.5 : 1.0\n",
      "                count(i) = 3                male : female =      4.5 : 1.0\n",
      "                count(w) = 1                male : female =      4.4 : 1.0\n",
      "                count(o) = 2                male : female =      3.5 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 12 : 0.702\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3753> 636 |\n",
      "  male | 1008<1547>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Ajai                          \n",
      "correct=male     guess=female   name=Ajay                          \n",
      "correct=male     guess=female   name=Lynn                          \n",
      "correct=male     guess=female   name=Randal                        \n",
      "correct=male     guess=female   name=Renaud                        \n",
      "\n",
      "Most Informative Features\n",
      "                 has(rv) = True             male : female =     24.6 : 1.0\n",
      "                 has(hu) = True             male : female =     22.3 : 1.0\n",
      "                 has(fo) = True             male : female =     21.6 : 1.0\n",
      "                 has(iu) = True             male : female =     16.6 : 1.0\n",
      "                 has(lt) = True             male : female =     16.1 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 13 : 0.754\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3998> 386 |\n",
      "  male | 1772 <788>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Chadd                         \n",
      "correct=male     guess=female   name=Jeremiah                      \n",
      "correct=male     guess=female   name=John                          \n",
      "correct=male     guess=female   name=Nestor                        \n",
      "correct=male     guess=female   name=Sylvan                        \n",
      "\n",
      "Most Informative Features\n",
      "           first2Letters = u'hu'            male : female =     18.5 : 1.0\n",
      "           first2Letters = u'wa'            male : female =     11.0 : 1.0\n",
      "           first2Letters = u'ya'            male : female =      8.4 : 1.0\n",
      "           first2Letters = u'wh'            male : female =      8.4 : 1.0\n",
      "           first2Letters = u'tu'            male : female =      8.4 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 14 : 0.656\n",
      "=========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = NLP_Classifier(nltk.NaiveBayesClassifier)\n",
    "# specify initial and final feature\n",
    "clf.fit_model(1, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of single features for Naive Baysien Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on results for features 1-14, we can see that the best performing features are; 1, 8, 9, 10, 11, 13. Combining these features, we produced features 15 & 16.  \n",
    "\n",
    "We will now evaluate classifier with these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3760> 631 |\n",
      "  male |  500<2053>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Gill                          \n",
      "correct=female   guess=male     name=Miriam                        \n",
      "correct=male     guess=female   name=Julian                        \n",
      "correct=male     guess=female   name=Noble                         \n",
      "correct=male     guess=female   name=Rollin                        \n",
      "\n",
      "Most Informative Features\n",
      "             last2letter = u'na'          female : male   =     90.6 : 1.0\n",
      "             last2letter = u'ra'          female : male   =     54.4 : 1.0\n",
      "             last2letter = u'ia'          female : male   =     38.1 : 1.0\n",
      "              lastletter = u'a'           female : male   =     38.0 : 1.0\n",
      "                 has(fo) = True             male : female =     36.1 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 15 : 0.81\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<1613>2766 |\n",
      "  male |   27<2538>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Blinny                        \n",
      "correct=female   guess=male     name=Geri                          \n",
      "correct=female   guess=male     name=Jo Ann                        \n",
      "correct=female   guess=male     name=Joycelin                      \n",
      "correct=female   guess=male     name=Ricki                         \n",
      "\n",
      "Most Informative Features\n",
      "          last2letter=na = True           female : male   =     89.9 : 1.0\n",
      "          last2letter=ia = True           female : male   =     37.7 : 1.0\n",
      "            lastletter=a = True           female : male   =     35.4 : 1.0\n",
      "          last2letter=ra = True           female : male   =     33.6 : 1.0\n",
      "          last2letter=sa = True           female : male   =     32.6 : 1.0\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.naivebayes.NaiveBayesClassifier'> using feature 16 : 0.614\n",
      "=========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit_model(15, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, we would consider feature 15 to build the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Decision Tree\n",
    "\n",
    "The second classifier we selected is the Decision Tree classifier.  \n",
    "\n",
    "Again, we will first use features 1 - 14 (single features) and examine how the model perform for each. These results will provide us with the basis to derive more complexe features to refine model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3590> 802 |\n",
      "  male |  840<1712>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Clo                           \n",
      "correct=female   guess=male     name=Dell                          \n",
      "correct=female   guess=male     name=Row                           \n",
      "correct=male     guess=female   name=Connie                        \n",
      "correct=male     guess=female   name=Godfree                       \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 1 : 0.748\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4170> 206 |\n",
      "  male | 2218 <350>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Quentin                       \n",
      "correct=male     guess=female   name=Forster                       \n",
      "correct=male     guess=female   name=Joachim                       \n",
      "correct=male     guess=female   name=Lucas                         \n",
      "correct=male     guess=female   name=Owen                          \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 2 : 0.64\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3882> 501 |\n",
      "  male | 2052 <509>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Tandy                         \n",
      "correct=male     guess=female   name=Arnie                         \n",
      "correct=male     guess=female   name=Bartholomeus                  \n",
      "correct=male     guess=female   name=Fraser                        \n",
      "correct=male     guess=female   name=Martainn                      \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 3 : 0.628\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4362>   6 |\n",
      "  male | 2565  <11>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Al                            \n",
      "correct=male     guess=female   name=Antony                        \n",
      "correct=male     guess=female   name=Jae                           \n",
      "correct=male     guess=female   name=Laurance                      \n",
      "correct=male     guess=female   name=Vlad                          \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 4 : 0.658\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4398>   . |\n",
      "  male | 2546   <.>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Harlin                        \n",
      "correct=male     guess=female   name=Llewellyn                     \n",
      "correct=male     guess=female   name=Partha                        \n",
      "correct=male     guess=female   name=Reinhold                      \n",
      "correct=male     guess=female   name=Siddhartha                    \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 5 : 0.598\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4369>   . |\n",
      "  male | 2575   <.>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Antonin                       \n",
      "correct=male     guess=female   name=Hasty                         \n",
      "correct=male     guess=female   name=Heath                         \n",
      "correct=male     guess=female   name=Pablo                         \n",
      "correct=male     guess=female   name=Wyatan                        \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 6 : 0.656\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4262> 120 |\n",
      "  male | 2419 <143>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Bev                           \n",
      "correct=male     guess=female   name=Gearard                       \n",
      "correct=male     guess=female   name=Jeffry                        \n",
      "correct=male     guess=female   name=Kingston                      \n",
      "correct=male     guess=female   name=Magnum                        \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 7 : 0.624\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3919> 486 |\n",
      "  male |  874<1665>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Avery                         \n",
      "correct=male     guess=female   name=Ely                           \n",
      "correct=male     guess=female   name=Enrique                       \n",
      "correct=male     guess=female   name=Hillery                       \n",
      "correct=male     guess=female   name=Marlowe                       \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 8 : 0.808\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<3903> 471 |\n",
      "  male |  512<2058>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Kary                          \n",
      "correct=male     guess=female   name=Allyn                         \n",
      "correct=male     guess=female   name=Hassan                        \n",
      "correct=male     guess=female   name=Murphy                        \n",
      "correct=male     guess=female   name=Shay                          \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 9 : 0.78\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4138> 229 |\n",
      "  male | 1163<1414>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Drew                          \n",
      "correct=male     guess=female   name=Arie                          \n",
      "correct=male     guess=female   name=Enoch                         \n",
      "correct=male     guess=female   name=Ernest                        \n",
      "correct=male     guess=female   name=Noland                        \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 10 : 0.736\n",
      "=========================================\n",
      "\n",
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4317>  69 |\n",
      "  male | 2333 <225>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=male     guess=female   name=Baillie                       \n",
      "correct=male     guess=female   name=Linoel                        \n",
      "correct=male     guess=female   name=Merrick                       \n",
      "correct=male     guess=female   name=Petey                         \n",
      "correct=male     guess=female   name=Tarzan                        \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 11 : 0.642\n",
      "=========================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "       |    f      |\n",
      "       |    e      |\n",
      "       |    m    m |\n",
      "       |    a    a |\n",
      "       |    l    l |\n",
      "       |    e    e |\n",
      "-------+-----------+\n",
      "female |<4150> 219 |\n",
      "  male | 1175<1400>|\n",
      "-------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "list of first 5 errors :\n",
      "correct=female   guess=male     name=Agretha                       \n",
      "correct=female   guess=male     name=Felipa                        \n",
      "correct=male     guess=female   name=Barri                         \n",
      "correct=male     guess=female   name=Lex                           \n",
      "correct=male     guess=female   name=Reinhold                      \n",
      "\n",
      "-----------------------------------------\n",
      "Accuracy of model <class 'nltk.classify.decisiontree.DecisionTreeClassifier'> using feature 12 : 0.716\n",
      "=========================================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1e3f9ebd886e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclf_dt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNLP_Classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# specify initial and final feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf_dt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-9e1331c2398f>\u001b[0m in \u001b[0;36mfit_model\u001b[1;34m(self, feat_num_start, feat_num)\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeat_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m             \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[1;31m# For errors list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Refine the stump.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         tree.refine(labeled_featuresets, entropy_cutoff, depth_cutoff-1,\n\u001b[1;32m--> 161\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# Return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mrefine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    201\u001b[0m                 self._decisions[fval] = DecisionTreeClassifier.train(\n\u001b[0;32m    202\u001b[0m                     \u001b[0mfval_featuresets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy_cutoff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_cutoff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m                     support_cutoff, binary, feature_values, verbose)\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             default_featuresets = [(featureset, label) for (featureset, label)\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             tree = DecisionTreeClassifier.best_stump(\n\u001b[1;32m--> 154\u001b[1;33m                 feature_names, labeled_featuresets, verbose)\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             tree = DecisionTreeClassifier.best_binary_stump(\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mbest_stump\u001b[1;34m(feature_names, labeled_featuresets, verbose)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[0mstump\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[0mstump_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstump\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstump_error\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0mbest_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstump_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, labeled_featuresets)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\vbrio\\Anaconda2\\lib\\site-packages\\nltk\\classify\\decisiontree.pyc\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m# Decision tree:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mfval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decisions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decisions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf_dt = NLP_Classifier(nltk.DecisionTreeClassifier)\n",
    "# specify initial and final feature\n",
    "clf_dt.fit_model(1, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, it would seem that features; 1, 8, 9, 10, 12 lead to better outcomes.  We will now evaluate the evaluate the combination features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_dt.fit_model(15, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection (Choose Best Candidate) \n",
    "\n",
    "Now that we have the best model build with both classifiers, we will test the model against the test data set and compare the results.  \n",
    "\n",
    "\n",
    "#### Check the model's final performance on the test set. \n",
    "\n",
    "#### How does the performance on the test set compare to the performance on the dev-test set? \n",
    "\n",
    "#### Is this what you'd expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
